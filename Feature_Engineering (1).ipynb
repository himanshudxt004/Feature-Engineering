{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a Parameter?\n",
        "\n",
        "A parameter is a numerical value that is learned by a machine learning model\n",
        "during the training process. These values control the behavior of the model0\n",
        "and determine how input data is transformed into output predictions. Parameters\n",
        "are not manually set; instead, they are automatically adjusted by the learning\n",
        "algorithm in such a way that the error between the actual output and the\n",
        "predicted output becomes minimum.\n",
        "\n",
        "For example, in a linear regression model represented by the equation:\n",
        "\n",
        "y = mx + b\n",
        "\n",
        "the values m (slope) and b (intercept) are parameters. During training, the\n",
        "model finds the most suitable values of m and b so that the predicted line fits\n",
        "the given data points as accurately as possible.\n",
        "\n",
        "Thus, parameters are internal to the model and are essential for making\n",
        "accurate predictions.\n"
      ],
      "metadata": {
        "id": "yfEvw7IXgm0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. What is Correlation?\n",
        "\n",
        "Correlation is a statistical measure that describes the relationship between two variables. It indicates how strongly and in what direction the variables are related to each other. Correlation helps in understanding whether a change in one variable causes a change in another variable.\n",
        "\n",
        "The value of correlation lies between -1 and +1.\n",
        "\n",
        "If the correlation is +1, it means there is a perfect positive relationship.\n",
        "If the correlation is 0, it means there is no relationship.\n",
        "If the correlation is -1, it means there is a perfect negative relationship.\n",
        "\n",
        "Correlation is widely used in data analysis and feature selection because highly correlated variables may contain similar information.\n",
        "\n",
        "# What does Negative Correlation mean?\n",
        "Negative correlation means that when the value of one variable increases, the value of the other variable decreases, and vice versa. In other words, both variables move in opposite directions.\n",
        "\n",
        "For example, as the price of a product increases, its demand decreases. Similarly, if the speed of a vehicle increases, the time required to reach the destination decreases.\n",
        "\n",
        "This inverse relationship between variables is known as negative correlation.\n"
      ],
      "metadata": {
        "id": "m9M1HTW1coHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Machine Learning is a branch of artificial intelligence that enables computers to learn from data and make predictions or decisions without being explicitly programmed. Instead of following fixed instructions, the machine identifies patterns from the data and improves its performance automatically with experience.\n",
        "\n",
        "The main components of machine learning are:\n",
        "\n",
        "1. Dataset - It is the collection of data used for training and testing the model.\n",
        "2. Features - These are the input variables used to make predictions.\n",
        "3. Target variable - This is the output that the model tries to predict.\n",
        "4. Model - It is the mathematical representation that learns patterns from the data.\n",
        "5. Loss function - It measures the error between actual and predicted values.\n",
        "6. Optimization algorithm - It updates the parameters to reduce the loss.\n",
        "7. Evaluation metric - It measures the performance of the trained model.\n"
      ],
      "metadata": {
        "id": "D946ZcgecocG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Loss value represents the difference between the actual output and the predicted output given by the model. It tells how well the model is performing.\n",
        "\n",
        "A small loss value indicates that the predicted values are close to the actual values, which means the model is performing well. A large loss value indicates poor performance.\n",
        "\n",
        "During training, the objective of the model is to minimize the loss function. If the loss continuously decreases, it means the model is learning. If the training loss is very low but the testing loss is very high, then the model is overfitting.\n",
        "\n",
        "Therefore, loss value is an important measure for determining the quality of a machine learning model."
      ],
      "metadata": {
        "id": "uapZ_nRqcojS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. What are Continuous and Categorical Variables?\n",
        "\n",
        "Continuous variables are numerical variables that can take any value within a given range. They are measurable quantities and usually represent real numbers. Examples include height, weight, temperature, and salary.\n",
        "\n",
        "Categorical variables are variables that represent categories or labels. They do not represent numerical quantities. Examples include gender, city, color, and department.\n",
        "\n",
        "Continuous variables are used directly in machine learning models, whereas categorical variables must be converted into numerical form before being used.\n"
      ],
      "metadata": {
        "id": "f35wGBBycopv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Machine learning algorithms work with numerical data, so categorical variables must be converted into numerical format. The common techniques are:\n",
        "\n",
        "**Label Encoding**\n",
        "In this method, each category is assigned a unique number. For example, Red = 0, Blue = 1, Green = 2. This method is suitable when the categories have an order.\n",
        "\n",
        "**One-Hot Encoding**\n",
        "In this method, new binary columns are created for each category. If the category is present, the value is 1; otherwise, it is 0. This method is used when there is no order among the categories.\n",
        "\n",
        "**Ordinal Encoding**\n",
        "This method is used when the categories have a meaningful order, such as Low, Medium, and High.\n",
        "\n",
        "These techniques help in converting categorical data into a format suitable for machine learning models.\n"
      ],
      "metadata": {
        "id": "vCD-X9tGcosP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. What do you mean by training and testing a dataset?\n",
        "\n",
        "Training dataset is the portion of the data that is used to train the machine learning model. The model learns the relationship between input features and the target variable using this data.\n",
        "\n",
        "Testing dataset is the portion of the data that is used to evaluate the performance of the trained model. This data is not shown to the model during training. It helps in checking how well the model works on new and unseen data.\n",
        "\n",
        "This division ensures that the model is not just memorizing the data but is actually learning the underlying patterns."
      ],
      "metadata": {
        "id": "6OTMv7XScouy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 8. What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module of the Scikit-learn library that is used for data preprocessing. It provides functions for transforming raw data into a suitable format for machine learning algorithms.\n",
        "\n",
        "It is used for:\n",
        "\n",
        "Feature scaling using StandardScaler and MinMaxScaler\n",
        "Encoding categorical variables using LabelEncoder and OneHotEncoder\n",
        "Normalization of data\n",
        "Handling binary data\n",
        "\n",
        "Data preprocessing is an important step because machine learning models perform better when the input data is properly scaled and formatted.\n"
      ],
      "metadata": {
        "id": "PclELLpBcoxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. What is a Test set?\n",
        "\n",
        "A test set is a part of the dataset that is used to evaluate the final performance of a trained machine learning model. It contains data that the model has never seen before.\n",
        "\n",
        "The test set helps in measuring how accurately the model can make predictions on new and unseen data. It is used only after the training process is complete.\n",
        "\n"
      ],
      "metadata": {
        "id": "PMc6Z8btco0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "In Python, data is commonly split using the train_test_split function from sklearn.model_selection.\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "Here, test_size = 0.2 means 20% of the data is used for testing and 80% for training.\n",
        "random_state ensures that the split is reproducible.\n",
        "\n",
        "## How do you approach a Machine Learning problem?\n",
        "\n",
        "The general steps to solve a machine learning problem are:\n",
        "\n",
        "1. **Problem understanding**\n",
        "   First, understand whether the problem is classification, regression, or clustering.\n",
        "\n",
        "2. **Data collection**\n",
        "   Gather the relevant data required to train the model.\n",
        "\n",
        "3. **Data preprocessing**\n",
        "   Handle missing values, encode categorical variables, and scale the features.\n",
        "\n",
        "4. **Exploratory Data Analysis**\n",
        "   Analyze the data using graphs and statistical methods to find patterns and relationships.\n",
        "\n",
        "5. **Feature selection and feature engineering**\n",
        "   Select important features and create new features if required.\n",
        "\n",
        "6. **Splitting the dataset**\n",
        "   Divide the dataset into training and testing sets.\n",
        "\n",
        "7. **Model selection**\n",
        "   Choose a suitable machine learning algorithm.\n",
        "\n",
        "8. Model training\n",
        "   Train the model using the training dataset.\n",
        "\n",
        "9. **Model evaluation**\n",
        "   Evaluate the model using appropriate performance metrics.\n",
        "\n",
        "10. **Hyperparameter tuning**\n",
        "    Improve the performance of the model by adjusting its parameters.\n",
        "\n",
        "11. **Prediction**\n",
        "    Use the trained model to make predictions on new data.\n"
      ],
      "metadata": {
        "id": "7BPH0Yuuco2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Exploratory Data Analysis (EDA) is performed to understand the structure, quality, and patterns present in the dataset before applying any machine learning algorithm. Real-world data is usually incomplete, noisy, and inconsistent. If we directly train a model without understanding the data, the model may learn incorrect patterns and produce poor results.\n",
        "\n",
        "EDA helps in:\n",
        "\n",
        "1. Understanding the distribution of data\n",
        "   It shows whether the data is normally distributed or skewed.\n",
        "\n",
        "2. Detecting missing values\n",
        "   Missing values must be handled before training.\n",
        "\n",
        "3. Identifying outliers\n",
        "   Outliers can significantly affect model performance.\n",
        "\n",
        "4. Checking relationships between variables\n",
        "   This helps in selecting important features.\n",
        "\n",
        "5. Detecting multicollinearity\n",
        "   Highly correlated independent variables can reduce model efficiency.\n",
        "\n",
        "6. Understanding the scale of features\n",
        "   Some models require feature scaling.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vEM41kYHg6R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12. What is Correlation?\n",
        "\n",
        "Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. It tells how one variable changes with respect to another.\n",
        "\n",
        "The correlation value ranges from -1 to +1:\n",
        "\n",
        "+1 -> Perfect positive correlation\n",
        "0 -> No correlation\n",
        "-1 -> Perfect negative correlation\n"
      ],
      "metadata": {
        "id": "QhMwh0RKhwHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13. What does Negative Correlation mean?\n",
        "\n",
        "Negative correlation means that when one variable increases, the other variable decreases. Both variables move in opposite directions.\n",
        "\n",
        "Example:\n",
        "As the price of a product increases, its demand decreases.\n"
      ],
      "metadata": {
        "id": "YlwC0VJXhwKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##14. How can you find correlation between variables in Python?\n",
        "\n",
        "Correlation in Python can be calculated using the pandas library.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "correlation_matrix = data.corr()\n",
        "print(correlation_matrix)\n",
        "```\n",
        "\n",
        "To find correlation between two variables:\n",
        "\n",
        "```python\n",
        "data[\"col1\"].corr(data[\"col2\"])\n",
        "```\n"
      ],
      "metadata": {
        "id": "_h4Z6UuThwT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation means that a change in one variable directly causes a change in another variable.\n",
        "\n",
        "Correlation only shows that two variables are related, but it does not prove that one causes the other.\n",
        "\n",
        "Example:\n",
        "\n",
        "Ice cream sales and number of drowning cases are positively correlated.\n",
        "But ice cream does not cause drowning.\n",
        "The actual cause is hot weather, which increases both ice cream sales and swimming activity.\n",
        "\n",
        "Correlation -> Variables move together\n",
        "Causation -> One variable produces an effect on another\n"
      ],
      "metadata": {
        "id": "hnU8TPCWhwWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "An optimizer is an algorithm used to minimize the loss function by updating the model parameters during training. It finds the best values of parameters so that the model makes accurate predictions.\n",
        "\n",
        "Types of optimizers:\n",
        "\n",
        "### 1. Gradient Descent\n",
        "\n",
        "It updates parameters by moving in the direction of the negative gradient of the loss function.\n",
        "\n",
        "Example:\n",
        "Used in linear regression.\n",
        "\n",
        "### 2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "It updates parameters using one training example at a time. It is faster and suitable for large datasets.\n",
        "\n",
        "### 3. Mini-batch Gradient Descent\n",
        "\n",
        "It updates parameters using small batches of data. It is the most commonly used method.\n",
        "\n",
        "### 4. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "It combines the advantages of Momentum and RMSProp. It is fast and widely used in deep learning.\n",
        "\n",
        "### 5. RMSProp\n",
        "\n",
        "It adjusts the learning rate automatically for each parameter.\n"
      ],
      "metadata": {
        "id": "ZSsXEClEhwZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##17. What is sklearn.linear_model?\n",
        "\n",
        "sklearn.linear_model is a module in Scikit-learn that contains all linear models.\n",
        "\n",
        "Examples:\n",
        "\n",
        "LinearRegression\n",
        "LogisticRegression\n",
        "Ridge\n",
        "Lasso\n",
        "\n",
        "These models are used for regression and classification tasks.\n"
      ],
      "metadata": {
        "id": "1lSRNOXqhwb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##18 What does model.fit() do? What arguments must be given?\n",
        "\n",
        "model.fit() is used to train the machine learning model. It learns the relationship between input features and the target variable.\n",
        "\n",
        "Syntax:\n",
        "\n",
        "```python\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Arguments:\n",
        "\n",
        "X_train ->Training input data.\n",
        "\n",
        "y_train -> Training output data.\n"
      ],
      "metadata": {
        "id": "EGGw-eCJhweq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "model.predict() is used to make predictions using the trained model.\n",
        "\n",
        "Syntax:\n",
        "\n",
        "```python\n",
        "model.predict(X_test)\n",
        "```\n",
        "\n",
        "Argument:\n",
        "\n",
        "X_test -> Data for which predictions are required\n"
      ],
      "metadata": {
        "id": "I4gHP74EhwhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20. What are Continuous and Categorical Variables?\n",
        "\n",
        "Continuous variables are numerical variables that can take any value within a range.\n",
        "Example: height, weight, salary.\n",
        "\n",
        "Categorical variables represent categories or labels.\n",
        "Example: gender, city, color.\n"
      ],
      "metadata": {
        "id": "kgnxN-qjhwje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##21. What is Feature Scaling? How does it help?\n",
        "\n",
        "Feature scaling is the process of bringing all feature values to a similar range.\n",
        "\n",
        "It is important because:\n",
        "\n",
        "1. It improves model performance\n",
        "2. It speeds up training\n",
        "3. It prevents features with large values from dominating\n",
        "4. It is required for distance-based algorithms like KNN and SVM\n"
      ],
      "metadata": {
        "id": "k1A5Pm6ZhwmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##22. How do we perform scaling in Python?\n",
        "\n",
        "Using StandardScaler:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "Using MinMaxScaler:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```"
      ],
      "metadata": {
        "id": "8SfJwiFdhwoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##23. What is sklearn.preprocessing?\n",
        "\n",
        "It is a module of Scikit-learn used for data preprocessing.\n",
        "\n",
        "It provides:\n",
        "\n",
        "StandardScaler\n",
        "\n",
        "MinMaxScaler\n",
        "\n",
        "LabelEncoder\n",
        "\n",
        "OneHotEncoder\n",
        "\n",
        "Normalizer\n",
        "\n",
        "It helps in converting raw data into a suitable format for machine learning.\n"
      ],
      "metadata": {
        "id": "Yk7-QF8khwrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##24. How do we split data for training and testing in Python?\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "test_size = 0.2 means 20% testing and 80% training.\n"
      ],
      "metadata": {
        "id": "ob9CDVBzhwuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##25. Explain Data Encoding\n",
        "\n",
        "Data encoding is the process of converting categorical data into numerical form so that it can be used in machine learning models.\n",
        "\n",
        "Types of encoding:\n",
        "\n",
        "### 1. Label Encoding\n",
        "\n",
        "Each category is assigned a unique number.\n",
        "\n",
        "### 2. One-Hot Encoding\n",
        "\n",
        "Creates separate binary columns for each category.\n",
        "\n",
        "### 3. Ordinal Encoding\n",
        "\n",
        "Used when categories have a meaningful order.\n",
        "\n",
        "Encoding is necessary because machine learning algorithms work only with numerical data.\n"
      ],
      "metadata": {
        "id": "-_sGDi3ehw3R"
      }
    }
  ]
}